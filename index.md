
*RWL = Read Watch Learn*

# Agust 1, 2024

I am back!

## Theory of uncertainty? 

What is the right theory of uncertainty--the right inference engine that allows us to form beliefs and reach conclusions about events, facts and hypotheses that are uncertain? Clearly probability theory has to play a role. Bayesianism is the theory? But that does not seem right. The main objection I see comes from **small worlds** or **unanticipated possibilities**. You calculate probabilities given a small world--say a game of chance, with fixed sample space and clearly defined rules governing the production of outcomes? Clearly probability applies there. And it is an excellent guide.

But you might have forgotten, not considered certain possibilities. Or these possibilities might not be salient to you at the time you construct your small world model relative to which you calculate the probabilities. So how can these probabilities be action-guiding? They are model relative. They say: If the world is like the model, then the chances are like this. But what if the world is *not* like the model? Then, what? Then, the probabilities can be wrong and get you astray. 

This might not seem a bit problem. Everything, after all, is model relative. If the model is wrong, then the reccomendations coming from the model are wrong. The same applies to data. I base my decisions on data, but the data can be wrong or fabricated or whatever, and if so, then I am in trouble if I follow the data! But this does not mean I should *not* follow the data. So what is the problem with probabilities then?

Well, the inferences and decisions we make are not made in a small world, but the real world---the *big* world. So a theory of uncertainity should operate in the big world, not in a small world. It would be an incomplete theory of uncertainty if it were limited to small worlds. So the problem is, Bayesianim is a good theory of uncertainty in small worlds, but what about the big world? 

This question becomes pressing as we look at other domain besides small worlds. What about law, finance, medicine or even personal life decisions? Even in science, say physics, it does not seem probability---Bayesianism---gives us a good account of what is going on. 

In **law**, the most penetrating critique of Bayesianism is by Ron Allen. I know his work, but it would be good to get his objections all in one place.

In **economics**, monetary policy and social science, this book seems good as a critique of Bayesianism:

- John Kay and Mervyn King (2020), [Radical Uncertainty](https://www.amazon.com/Radical-Uncertainty-Decision-Making-Beyond-Numbers/dp/1324004770)

In philosopbhy of **science**, here is a survey article:

- John Norton, [Challenges to Bayesian confirmation theory](https://sites.pitt.edu/~jdnorton/papers/Challenges_final.pdf)

How do **financial planners** make decisions? What guiding principles of uncertainty do they follow? Are they just guided by probabilities and expected utility?

# August 6, 2024

## Rootclaim debate about the origins of Covid-19

Rootclaim holds that covid spread because of a lab leak. It bet usd 100,000 this was the right claim and anyone who wanted to challange them was free to do it. Peter Miller did and [won](https://blog.rootclaim.com/rootclaims-covid-19-origins-debate-results/)! He defended the view that covid came from animals, possibly from bats to civets sold in the Wuhan wildlife market. I've started watching the debate. It is very interesting.

The debate is relevant for my earlier question, what is the right theory of uncertainty---the right inference engine that allows us to form beliefs and reach conclusions about events, facts and hypotheses that are uncertain? Rootclaim did a probabilistic analysis. Peter Miller, instead, did what we might call a fact-focused narrative analysis. He collected as many facts as reasonably available, put them together, compared the strengths of the two competing theories (animal origin versus lab leak) and concluded that the lab leak theory had to posit many more coincidences than the animal origin theory. It seems Peter Milller thought the animal origin theory explains the evidence/facts we have better than the lab leak theory does. Rootclaim might not disagree with that, but still hold that the lab leak theory is more probable overall. So what is the criterion to follow? Explanatory power or likelihood of truth? 

One of the judges in the debate wrote (in a blog [entry](https://ermsta.com/posts/20240301)) probability theory is not the right framework to assess the evidence. He still provided a probabilistic analysis, but did not think it was dispositive:

> I was concerned that people might interpret my Bayesian analysis as the “main product” of the report, in isolation from the rest, which is why I heavily cautioned against taking it too literally; indeed I explicitly state that I do not think it is an appropriate technique for this problem, and am only producing one because it feels fair to do so after criticizing Rootclaim’s analysis.

Interesting! His very detailed analysis is below:

- Eric Stansifer, [Rootclaim covid-19 origins debate: Final decision](https://ermsta.com/r/covid_decision_20240217.pdf)

I need to read it carefully, and see why Eric thought Bayesianism is not the right (or not the ultimate?) guide for 
judging the debate about the origins of covid. But the clash between explanatory power versus and likelihood of a story is reminiscient of debates in legal scholarship, between legal probabilists/Bayesians and Ronald Allen who prefers relative plausibility. 

## ACLU on Trump Excutive Order on DEI and Florida HB7 bill

ACLU wrote a memo about [Trump on DEI and anti-discrmination law](https://assets.aclu.org/live/uploads/2024/07/Memo_Trump_DEI_v4.pdf). It is a good read, informative and well-researched, but I was confused by some of the presentation, tone and overall argument.  Consider this claim:

> A second Trump administration would supercharge efforts to censor discussion of any concepts deemed
“divisive” from the nation’s classrooms, by which it means classroom discussions about race, gender, and systemic
oppression with which it disagrees. (p. 2).

What the ACLU claims here might be true as a matter of fact---that is, this might very well be what Trum has actually in mind---but we should look at the evidence and the historical record. So what comes to mind is the executive order banning DEI (actually, [Executive Order Combating Race and Sex Stereotyping](https://trumpwhitehouse.archives.gov/presidential-actions/executive-order-combating-race-sex-stereotyping/) issued in September 2020). In it (Section 2), we find a list of "divise concepts" . Here it is:

> (a) “Divisive concepts” means the concepts that (1) one race or sex is inherently superior to another race or sex; (2) the United States is fundamentally racist or sexist; (3) an individual, by virtue of his or her race or sex, is inherently racist, sexist, or oppressive, whether consciously or unconsciously; (4) an individual should be discriminated against or receive adverse treatment solely or partly because of his or her race or sex; (5) members of one race or sex cannot and should not attempt to treat others without respect to race or sex; (6) an individual’s moral character is necessarily determined by his or her race or sex; (7) an individual, by virtue of his or her race or sex, bears responsibility for actions committed in the past by other members of the same race or sex; (8) any individual should feel discomfort, guilt, anguish, or any other form of psychological distress on account of his or her race or sex; or (9) meritocracy or traits such as a hard work ethic are racist or sexist, or were created by a particular race to oppress another race. The term “divisive concepts” also includes any other form of race or sex stereotyping or any other form of race or sex scapegoating.

If you did not know this list came from the infamous executive order, you probably would disagree with most, if not all the items in the list. Who would agree that (1) one race is superior to another? Only racists would. Who would agree that (3) people, just becaue of their race, are racist? Who would agree that (4) people should be discriminated against because of their race? Who would agree that (6) one's moral character is determined by their race? And so on. Many (all?) of these claims are certainly "divisive" and quite simply morally abhorrent.

The divisive concepts listed in the exective order are not "classroom discussions about race, gender, and systemic
oppression", as the ACLU memo suggests. They are, as the order names them, divisive. Now, as I said, Trump might actually want to prohibit any discussion about race, racism or structural oppression. Some of his voters might want that. But this is not what the executive order says. So the framing of the issue by the ACLU seems misleading. 

After the executive order came out in September 2020, the Berkeley Center of Othering and Belonging--whose mission "to build a world where everyone belongs" sounds great---[commented](https://belonging.berkeley.edu/why-trumps-diversity-training-ban-unconstitutional) as follows:

> The Executive Order grossly misrepresents the nature of Diversity, Equity and Inclusion training. As someone who has conducted or helped prepare countless such trainings, I have never made such claims or anything close to such claims. Most of my training materials include data, facts, figures, laws and policies, and recommendations for reform or policy change. In fact, saying that a person is inherently superior because of their race or sex is not only antithetical to the concept of equity, it is one of the definitions of racism. It would be absurd for an antiracist training to trade in such claims.

If no DEI practioner would push the divise concepts in the executive order, the order does not target DEI initiatives, and DEI practioners have nothing to fear, right? Some will say this conclusion is naive. Granted, the order likely has a chilling effect. DEI practioners for fear of litigigation against them might self-censor even though they do not actually profess divisive concepts. This is certainly a risk that should not be overestimated. 

Still, on its face, the order targets not DEI as such, but at most DEI so long as it is divisive. The question becomes an emprical one. Do DEI practioners actually push diviside concepts? If they don't, as the Berkeley Center of Othering and Belonging attests, the order on its face cannot target DEI. 

Let's now look at the Florida HB7 which has a similar intent. The key bit is this:

> 760.10 Unlawful employment practices.—
> 8)(a) Subjecting any individual, as a condition of
> employment, membership, certification, licensing, credentialing,
> or passing an examination, to training, instruction, or any
> other required activity that espouses, promotes, advances,
> inculcates, or compels such individual to believe any of the
> following concepts constitutes discrimination based on race,
> color, sex, or national origin under this section:

This paragraph is then followed by a list of divisive concepts or claims, mostly verbatim 
from the executive order. So no need to repeate them here.

But there are a couple of interesting twists. The first:

> (b) Paragraph (a) may not be construed to prohibit
discussion of the concepts listed therein as part of a course of
training or instruction, provided such training or instruction
is given in an objective manner without endorsement of the
concepts.

This is, on its face, reassuring. Discussions about race, discrimination, racism, etc. are not banned.
The point is to make sure these discussions do not push divisive concepts. In another section, in fact, the H7B bill describes a few discussion topics in the school curriculum and lists this:

> Instructional personnel may facilitate
> discussions and use curricula to address, in an age-appropriate
> manner, how the individual freedoms of persons have been
> infringed by slavery, racial oppression, racial segregation, and
> racial discrimination, as well as topics relating to the
> enactment and enforcement of laws resulting in racial
> oppression, racial segregation, and racial discrimination

This is, again, reassuring. The bill explitly states that instrucutors may cover topics such as racism, discrimination and racial oppression.  This is far from, as the ACLU put it, a ban on "classroom discussions about race, gender, and systemic
oppression". The potential chilling effect remains, but that is a different point. 

*UPDATE*: ACLU Florida describes in a 2022 [note](https://www.aclufl.org/sites/default/files/field_documents/aclu_fl_written_testimony_in_opposition-_hb_7_government_censorship_of_race_gender_discussions_judiciary_1.25.22.pdf) the HB7 like this:

> HB 7/SB 148 make it an unlawful employment practice for any employer
to engage in discussions, instruction, or training about race, gender,
national origin, and the impacts of slavery and patriarchal systems.

Quotations above from HB7 itself shows this isn't accurate. The bill itself invites
"discussions, instruction, or training about race" (well, so long as they do not push divisive concepts which, as we know, 
are not part of DEI trainings). So why this misrepresentation? 


# August 7, 2024

## Predicting the economy -- beyond standard economic modeling 

This seems like a great book to read:

- RWL: Farmer (2024), [Making Sense of Chaos: A Better Economics for a Better World](https://www.penguin.co.uk/books/284357/making-sense-of-chaos-by-farmer-j-doyne/9780241201978)

The key thesis. Standard economics work with mathematical models that rest of simple and artificial assumptions of rationality (say, maximization of expected utility).  These models are idealizations and have no traction in the real world. So economics predictive power is weak. What is the alternative? Simulate! Collect data about every relevant aspect in the economy---employment, production, immigration, etc. data we have have more and more. Then, simulate agents and processes. Let the simulation run over time and see what happens. Intriguing yet also distopyc and scary.

Farmer in [this excellent interview](https://www.youtube.com/watch?v=cIQP0hJ920k) provides several interesting examples of how his simulations have actually worked and helped central banks and governments make decisions:

- RWL: simulation of Washington housing market - [paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4710928)
- RWL: simulation of UK housing market for the Bank of England - [paper](https://www.bankofengland.co.uk/working-paper/2016/macroprudential-policy-in-an-agent-based-model-of-the-uk-housing-market)
- RWL: simulation of UK economy during the pademic - [paper](https://www.sciencedirect.com/science/article/pii/S0165188922002317) and [newspaper piece](https://csh.ac.at/news/predicting-the-economic-downturn-during-the-pandemic/)

He claims he managed to accurately predict a drop in UK GDP during the pandemic! This is all very impressive. This simulation approach, however, is resisted by mainstream economics. 

Farmer has a company -- [Macrocosmos](https://www.macrocosm.group/)that uses simulations to make predictions useful for policymakers, businesses, financial analysts, etc., with a focus on the climate change transition and the decarbonization of the economy. 

# August 21, 2024

## What is equality before labor?

Chapter 9 of [Equality: The History of an Elusive Idea](https://www.amazon.com/Equality-History-Darrin-M-McMahon/dp/0465093930) discusses how Italian fascism and German nazism were socialist movements interested in establishing equality. But what equality, exactly? 

Liberalism championed equality before the law. Italian fascism champions equality before labor. See 1927 [Carta del Lavoro](https://polyarchy.org/basta/documenti/carta.lavoro.1927.html):

> IV. Nel contratto collettivo di lavoro trova la sua espressione concreta la solidarietà fra i vari fattori della produzione, mediante la conciliazione degli opposti interessi dei datori di lavoro e dei lavoratori e la loro subordinazione agli interessi superiori della produzione.

> VI. Le associazioni professionali legalmente riconosciute, assicurano l'uguaglianza giuridica tra i datori di lavoro e i lavoratori, mantengono la disciplina della produzione e del lavoro e ne promuovono il perfezionamento.

This is hard to understand, but recent suggests economic inequalities did increase during Italian facism:

- Giacomo Gabbuti (2020), [A Noi! Income Inequality and Italian Fascism: Evidence
from Labour and Top Income Shares](https://ora.ox.ac.uk/objects/uuid:eeddbb9c-7b39-4347-91ff-fa25fa2c44bf/files/scc08hg081)

Interestingly, [Corradi Gini](https://en.wikipedia.org/wiki/Corrado_Gini) -- from the Gini coefficient -- was a fascist!



# Agust 26, 2024

## Pitfalls of the Bayesian analysis in the covid origins Rootclaim debate

I'll follow up on some earlier thoughts about the right theory of uncertainty, especially in relation to the Rootclaim debate about the origins of covid. See earlier post on August 6, 2024. 

One of the judges wrote a detailed report in which he claimed that Bayesianism is not the right theory to handle uncertainty. 

- Eric Stansifer, [Rootclaim covid-19 origins debate: Final decision](https://ermsta.com/r/covid_decision_20240217.pdf)

### Problem 1: Biased selection of the evidence 

Consider this scenario (pp. 20-21):

> Suppose you suspect something about your senator is quite odd, and perhaps they are not human at all, but rather secretly a lizard from space. To test this, you collect 100 pieces of data that inform the question. For many of these data points you know what to expect for humans: how tall are they, how much do they weigh, shapes of facial features, etc.

> Predictably, even if you have been completely fair in your data collection – all 100 data points are accurate, and you did not specifically seek out information that you know will be odd – if you are naive with your analysis in certain ways you will invariably conclude that they are a lizard.

How so? Well, you could pick one feature that happens to have a low p-value, then conclude it is evidence against the null (=not a lizard). This is known as p-hacking: look hard enough and you'll find evidence against the null. 

Very well, but what about Bayesian reasoning?

> Bayesian reasoning is capable of exactly the same error. For each of these observations you compute a Bayes factor. Half of them are less anomalous than average (Bayes factor 1) and half of them are more anomalous than average. They are only weakly anomalous – a 1 in
2 coincidence each, so Bayes factor 2 – but with 50 such pieces of weak evidence your total Bayes factor is 2^{50}, totally overwhelming any prior you had against secret space lizards.

The general point is clear, but the discussion is odd. Presumably, we'll have some evidence for Lizardness (in short, L) and some evidence against it. Say 50 features favor L and 50 do not. So the balance of evidence should be even. If the prior for L are low, the posterior probability of L will be low. Or most features could go against L, so the evidece against L will be overwhelming.   

The quotation instead assumes that 50 features could be weakly anomalous, with Bayes factor of 2. So, combined, the Bayes factor is 2^{50}, which is overwhelming evidence for L. But what about the other features? Why assume that their Bayes factor is merely 1? Why not 1/2 so that, combined, we have 1/2^{50}? The author admits that much when he writes (p. 21):

> However let us be a little less reckless; after all, all of those less anomalous observations
should have had a Bayes factor smaller than 1 because they are evidence for humanity.

Then, Stansifer assumes that we find out about these feature somewhat randomly and so their associated Bayes factors are uniformly distributed in the interval [0, 1].  So, some features will favor L and others go against L. In the end, aggregating them, we will either have very strong evidence for L or strong evidence against L. This is odd. We cannot assume that values of teh Bayes factor are picked randomly from a uniform interval [0, 1]. Evidence isn't produced randomly so long as it tracks reality. The mechanism that generates the evidence must be "biased" for or against L -- that will depeden on what reality is like, or erlse evidence would be worthless.

Next, Stansifer adds (p. 22):

> However, what happens if we have a slight bias? Indeed even if all of our data points are individually fair, we might be biased about which data points we collect. Suppose we conveniently overlook half the data in favor of humanity; so we have two thirds
for lizards, and one thirds for humans.

The point here is clear, and I completely agree. (The earlier discussion seemed muddled, or I could not follow it.) If we are biased in the selection of the evidence -- without knowing it! -- the evidence might support conclusion L very strongly, while in fact that might be due, not to the truth of L, but to the biased selection of the evidence.

This point is sensible, but it isn't a problem for Bayesianism per se. Any formalism or method to assess evidence will face this problem. So we have:

(Task 1) assess the *available* evidence foir/against a hypothesis H
(Task 2) assess whether evidence collection was fair, unbiased, etc.

Bayesianism can carry out Task 1, but clearly cannot carry out Task 2, and we should be wary of that. What are some remedies? Here is Stansifer again (p. 22):

> If instead of a haphazard collection of observations one makes a collection of evidence that is in some way canonical it is much harder to introduce bias; say, one considers all fingers, but only fingers.

> ... if a hypothesis can be broken down into subparts, each necessary for the hypothesis as a whole to hold, then there is less potential for bias in the selection of evidence, though there may remain disagreement about the numbers assigned to the evidence or what
breakdown into subparts is cleanest.

These are very good points, but none would be incompatible with Bayesianism. Yes, we should agree on types of evidence to look at beforehand, prior to knowing their content. For example, in a trial, to questiokn a witness prior to knowing what they are going to say; or run a test prior to knowing the test results; etc. This guards us againt biased selection of evidence. Not a problem for Bayesian, but also, not a problem that Bayesianism can address. 

Later in critizing (p. 26) one of Rootclaim calculations, Stansifer points out that -- even granting that the probabilities assigned to assess the evidence are right  -- the choice if the evidence seemed at hoc and thus not convicing:

> even if all the numbers shown here are completely correct, I am not persuaded by the conclusion. The listed observations do not represent a canonical series of necessary events that must occur under the hypothesis .... one could argue that the choice of observations was ... unmotivated. Rather, the listed observations appear to be a miscellany of loosely related things that happened in the context of the HSM outbreak and seemed vaguely odd.

I have mixed feelings about this point. First, it is odd to dismiss evidence because it is not canonical or not what one woud expect. Evidence can be shown to be irrelevant or fabricated, but dismissing it is odd. Second, evidence is presented by both parties in the debate. The adversarial format should take care of possible biases in evidence selection, at least assuming one party has no better access to evidence than the other party (and there is no reason for thinking that in the Rootclaim debate). Third, the task is to assess the likelihood of hypotheses based on the evidence available or presented, not based on what evidence should have been presented but was not. On the other hand, I see the point: a biased selection of the evidence could lead to the wrong results. 

I wonder, what was the point of the debate? To determine the truth of the lab leak hypothesis versus animal origin hypothesis, or rather, to assess what the evidence says about these hypotheses?

### Problem 2: Recklessly aggregating weak evidence

The second pronblem -- related to the first -- is that Bayesianism allows for reckless aggregation of weak evidence that can yield overwhelming evidence. Why is this a problem?

Stansifer  comments on Rootclaim Bayesian computation, summarized in the table on page 27. Basically, we have a bunch of items of evidence, each with a Bayes factors (greater or smaller than 1), which are then aggregated. Total Bayes factor is very high, favoring lab leak. Here again (p. 27):

> Half of that weight comes from three pieces of evidence: the 12nct insert, CGGCGG , and the location of the first SSE (sum 8.882). These three pieces of evidence are claimed to be strong and deserve commensurate analysis. The other half comes from eight different pieces of evidence that are individually weak and appear ad hoc.

> This argument ... bears some resemblance to the space lizard argument, and observations seem to be selected primarily on the basis of how much they stand out rather than any canonical or complete collection of evidence. This is fine if the observations really stand out – a factor of 50 is not bad, and you will be hard pressed to turn up many factors of 50 by chance. But Rootclaim includes five factors of 2 or less in their favor, and these should be plentiful to find for any hypothesis, true or false.


This is again the earlier point about possible biases in the selection of the evidence. But there is a new important point being made here. That is, it is okay to select very strong evidence in favor of one's hypothesis. But when the individual items are weak evidence, this is more worrisome, becaue as we know, even weak pieces of evidence when combined, can become overwhelming. And this may be totally artificial. 

Not sure how to develop this further, but worth thinking about more. It seems that for Bayesians, many pieces of weak evidence versus one piece of strong evidence would be indistinguishable (assuming independence).  Stansifer thinks not. The risk of evidence selection bias with weak evidence is greater than with one single, strong item of evidence. 

Here are some fixes by Stansifer (p. 29):

> 2. Focus on only the most extreme observations: it is harder to introduce a bias with 3 factors of 100 each than 20 factors of 2 each. Of course, this requires having extreme observations available.

3. Alternatively, use some canonical or clearly unmotivated selection of evidence. Rootclaim attempts to do this by using all the evidence, which certainly would be a canonical selection if it were possible; in practice I believe Rootclaim is aiming to include all
the relevant evidence, which is much harder to do without human bias interfering.


### Problem 3: Arbitrariness and lack of an explanation

The third problem is not stated in the report, and it is my own. In reading the report, I was struck by how brittle the Bayesian analysys can be. One can change dramatically the outcome by (1) adding or removing evidence and (2) adjusting the Bayesian facfor for each piece of evidence. See the difference between the Rootclaim table on page 27 and Stansifer's own calculations on page 67. In the former, the Bayesian factor for the lab leak is overwhelming, while in the latter, it is as low as 0.00003. How can we have such dramatic difference? This is somewhat disconcerting. 

## Can the Bayesian analysis help?

Rootclaim believes the Bayesian analysis can help. How? 

I don't think it can help much if pieces of evidence can be aggregated libearly or additively, like this: E1 favors H1; E2 favors H1; E2 favors not-H; E2 favors not-H, etc. At this level, we can count things and get a sense of where the evidence is going. The hard part is to collect the evidence and assess it. Which the Bayesian analysis does not help us do. The aggregation is easy. Part of the problem in teh rootclaim debate was that most of the discussion seemed to aggregate pieces of evidence in an additive manner. 

But one could have multiple sources of uncertainty at different levels. Say DNA evidence shows a match, but the uncertainty coud be due to (a) a laboratoty error and also (b) a random coincidence match (say, two people could have the same genetic profile). It is not obvious how to aggregate these two sources of uncertainty, especially when they are numeric.

In addition, imagine a witness gives an alibi in defense of the defendant. How do you add this to the overell body of evidence including DNA match? That is even less clear, because the witness speaks directlly to the issues of guilt/innocence while DNA evidence only speaks to the issue of presence at the scene or source of crime traces. The aggregation cannot be linear or additive. This kind of complexity can only be handled by probability and Bayesian networks.

# August 30, 2024

## Incredible coincidences

### Case 1: Lucia de Berk (NL)

Well known case, but interesting to ask, based on the data available, how to best come up with a p-value. Use hypergeometric distribution, binomial, poisson, or what elese?

### Case 2: Daniel Poggiali (IT)

Similar to Lucia de Berk, but a crazy series of twists and turns:

-  March 2016, Daniela Poggiali was sentenced by the Ravenna Court of Assizes to life imprisonment
-  July 7, 2017, the Court of Appeal of Bologna acquitted her1, overturning the
-  In 2018 the Supreme Court (Corte di Cassazione) annulled the sentence (“cassation”) and ordered a new trial
-  In 2019, she was acquitted again
-  In 2020 the Supreme Court ordered yet another new trial
-  In two subsequent appeals, she was acquittedm, but with both acquittals overridden by Cassation
-  October 2021, the Court of Appeal of Bologna acquitted Daniela yet again of murder charges
-  etc.

Dotto, Gill and Mortera (2022), [Statistical Analyses in the case of an Italian nurse accused of murdering patients](https://arxiv.org/pdf/2202.08895)

Look at the death data with DP. Just a coincidence?

| Nurse | Same Zone | Opposite Zone | Total Deaths | Hours on Duty | Same Zone Mortality Rate | Opposite Zone Mortality Rate | Relative Risk | Absolute Risk |
|-------|-----------|---------------|--------------|---------------|--------------------------|-----------------------------|---------------|---------------|
| N.1   | 68        | 58            | 126          | 3686          | 0.54                     | 0.46                        | 1.17          | 0.08          |
| N.2   | 51        | 68            | 119          | 3545          | 0.43                     | 0.57                        | 0.75          | -0.14         |
| N.3   | 64        | 60            | 124          | 3554          | 0.52                     | 0.48                        | 1.07          | 0.03          |
| N.4   | 70        | 53            | 123          | 3535          | 0.57                     | 0.43                        | 1.32          | 0.14          |
| N.5   | 64        | 41            | 105          | 3625          | 0.61                     | 0.39                        | 1.56          | 0.22          |
| N.6   | 43        | 65            | 108          | 3532          | 0.40                     | 0.60                        | 0.66          | -0.20         |
| DP    | 139       | 52            | 191          | 3577          | 0.73                     | 0.27                        | 2.67          | 0.46          |
| N.8   | 60        | 44            | 104          | 3710          | 0.58                     | 0.42                        | 1.36          | 0.15          |
| N.9   | 66        | 53            | 119          | 3741          | 0.55                     | 0.45                        | 1.25          | 0.11          |

### Case 3: Lucy Letby (UK)

Nurse recently convicted, again stats similar to Lucia de Berk

The Guardian (2024), [Lucy Letby: killer or coincidence? Why some experts question the evidence](https://www.theguardian.com/uk-news/article/2024/jul/09/lucy-letby-evidence-experts-question)

Richard Gill (2023/24), [The Lucy Letby case](https://gill1109.com/2023/05/24/the-lucy-letby-case/?amp=1)

### RSS Stats and Law report

- RSS (2022)[Healthcare serial killer or coincidence?](https://rss.org.uk/RSS/media/File-library/News/2022/Report_Healthcare_serial_killer_or_coincidence_statistical_issues_in_investigation_of_suspected_medical_misconduct_Sept_2022_FINAL.pdf)

### Pearson chi-squared statistic

Interesting to look at *Appendix 6: Patterns of occurrence of adverse events* (p. 45)

   
|                    | Nurse A | Nurse B | Total |
|--------------------|---------|---------|-------|
| **Died**           | 15      | 9       | 24    |
| **Survived**       | 25      | 31      | 56    |
| **Total**          | 40      | 40      | 80    |

> Could the apparent discrepancy in rates of death be attributed to chance, “just a coincidence”? We
> suppose that all circumstances of the Nurse A and Nurse B shifts are identical; there is no other
> conceivable reason for the apparent difference other than the presence of one nurse or the other.

> ... the standard way to analyse this, to test the hypothesis that there is no difference in the death rates attributable
> to the nurses, is “Pearson's chisquared test”. This is an elementary technique... This reveals that the probability of observing
> a difference in apparent death rates as large as, or larger than, that seen in the table, if there was really no difference
> is 14% (that is the pvalue is 0.14).

|                    | Nurse A | Nurse B | Total |
|--------------------|---------|---------|-------|
| **Died**           | 150     | 90      | 240   |
| **Survived**       | 250     | 310     | 560   |
| **Total**          | 400     | 400     | 800   |

> if all of the numbers in Table 5 were exactly 10 times larger (150, 90, and so on), then
> the Pearson chi-squared statistic 𝐺 would be 21.43 and the p-value turns out to be 0.000004,
> so there would be overwhelming evidence that the apparent different in death rate was not due to chance.
> (This is an example of the point made in Section 4(f) that “coincidental fluctuations from
> population means are more likely with small samples…”).

### Poisson log-linear models


> Pearson’s chi-squared test, but still very few criminal cases are simple enough to fit into this setting.
> Nevertheless, the analysis can be extended to deal with much more complex situations, allowing in
> particular more than one causal factor, and different durations of time. The more general framework is
> that of Poisson log-linear models, which are an example of generalised linear models. (p. 46)


| Number of Shifts | Nurse Shift       | Deaths | Expected Deaths Ignoring Morning Effect | Expected Deaths Allowing Morning Effect |
|------------------|-------------------|--------|------------------------------------------|------------------------------------------|
| 8                | On duty morning   | 7      | 5.33                                     | 7.87                                     |
| 7                | On duty other     | 3      | 4.67                                     | 2.13                                     |
| 2                | Off duty morning  | 2      | 0.40                                     | 1.13                                     |
| 28               | Off duty other    | 4      | 5.60                                     | 4.87                                     |


 > If we ignore the shift effect, we are simply comparing the rates of 10 per 15 shifts with 6 per 30 shifts
> when the nurse is or is not on duty. The Poisson log-linear analysis (details explained in Appendix 6, with code in Appendix 8)
> gives a p-value of 1.7% (0.017) for the likelihood ratio test that the nurse’s presence has no effect on the rates –
> and we would conventionally call this result significant, which would be incriminating. (p. 47)

>  if we follow the correct practice of comparing hypotheses (1) and (2) above, the p-value
> becomes 37.8% (0.378) ... not statistically significant (p. 47)

> To correctly assess the extent to which the deaths can be attributed to the presence
> of the nurse we must compare two hypotheses:
> (1) that the only cause of systematic difference in rates is the shift effect, and
> (2) that both the shift effect and the presence of the nurse have a systematic effect on the rates. 

Details of calculations on pp 51 and 52

# August 30, 2024

## Nurses and statistical coincidences

### Case 4: Beverly Allitt (UK)

This is a case of a conviction (May 1993), partly based on statistics, but there seems to have been more detailed/specific evidence, though read this:

-  Marks and  Richmond (2008), [Beverly Allitt: the nurse who killed babies](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2270251/)
  
> By the time I was consulted, the police had, by examining the nursing time sheets, established statistical evidence linking Allitt with each of the
> suspect hospital cases. There was a remarkable coincidence between the times when she was on duty and the occurrence of unexpected events in all 13 children.
> The chance of this being **coincidence was calculated to be less than one in 10 million**. In ten of the cases, the child's illness could have had a perfectly
> natural explanation – and even in the cases of Paul Crampton, Claire Peck and Becky Phillips there was an extremely remote possibility that the damage was accidental.
> It was only **when all the cases were taken together** and had been investigated in greater detail than had originally been considered necessary
> for clinical purposes that the whole thing fell into place.

So, the individual deaths, considered indvidually, did not raise suspicion, but considered together, they did!
Is this case any different from the others?

The other curious thing is that Roy Meadow -- the same guy who testified against Sally Clark by multiplying the statistics-- was one an expert witness in the trial. 

- [Beverly Allitt Crime Files](https://www.crimeandinvestigation.co.uk/crime-files/beverly-allitt)
  
> Her unusual behaviour in childhood was brought to light and the paediatrics expert, Professor Roy Meadow, explained Munchausen’s syndrome and Munchausen’s by Proxy syndrome
> to the jury, pointing out how Allitt demonstrated symptoms of both, as well as introducing evidence of her typical post-arrest behaviour, and high incidence of illness,
> which had delayed the start of her trial. It was Professor Meadows’ opinion that Beverley Allitt would never be cured, making her a clear danger to
> anyone with whom she might come in contact.

Meadow did not testify about the stastical coincidence (unlike in Sally Clak), but about the fact that the defendant fits a profile whoch makes her likely to have a motive to kill. 
So here we seem to have a case of stastical coincidence plus profiling!

### Case 5: Charles Cullen (US)

- [Interviews with serial killers | 60 Minutes Full Episodes](https://www.youtube.com/watch?v=c2xMTTmZpSM)

- This seems a very different type of case, defendant clearly guilty.

However, this is also a cae of statistical coincidences. Read here:

- [How Did Detectives Start Investigating 'The Good Nurse" Case?](https://www.oxygen.com/true-crime-buzz/how-detectives-tim-braun-danny-baldwin-investigate-charles-cullen)

> The press release added, “The case against Charles Cullen grew as the detectives discovered that at each facility an **inordinate number of unexplained deaths**
> had been recorded. Furthermore, 67% of the deaths occurred in the Critical Care units during the midnight shift when Charles Cullen was on duty.”

>  The two detectives then investigated the hospital’s automated medication system to confirm that Cullen had been removing unauthorized medications
> “at an alarming rate,” according to the press release. One such medication was Digoxin, the same drug which had killed at least one victim at that hospital.

So what is the difference from Lucia de Berk or similar other cases?

**QUESTION**: Do the statistics/data about deaths look any different in Cullen than they look in Lucia de Berk? Has someone done a statistical analysis? What is the p-value for the pattern of deaths in Cullen versus Lucia de Berk?

## Fingerprint ID and AI

-  Guo et al (2024), [Unveiling intra-person fingerprint similarity via deep contrastive learning](https://www.science.org/doi/10.1126/sciadv.adi0329)

- [Github page](https://github.com/gabeguo/FingerprintMatching/tree/refactor_training)

First finding: Fingerprints (from the same person) are not unique:

> Fingerprint biometrics are ... based on the unproven assumption that no two fingerprints, even from different fingers of the same person, are alike.
> This renders them useless in scenarios where the presented fingerprints are from different fingers than those on record. Contrary to this prevailing assumption,
> we show above 99.99% confidence that fingerprints from different fingers of the same person share very strong similarities. Using deep twin neural networks to
> extract fingerprint representation vectors, we find that these similarities hold across all pairs of fingers within the same person, even when controlling
> for spurious factors like sensor modality.

Second finding: ridge orientation in the center/singularity, not minutiae, is reponsible for the similarity:

> We also find evidence that ridge orientation, especially near the fingerprint center, explains a substantial part of this similarity, whereas minutiae
> used in traditional methods are almost nonpredictive. Our experiments suggest that, in some situations, this relationship can increase forensic
> investigation efficiency by almost two orders of magnitude.

Magazine piece on the scholarly article with a critique by Simon Cole:

- [Forensic scientists have a new fingerprint-matching tool in their arsenal thanks to AI, but it's sparked a controversy News](https://www.livescience.com/technology/artificial-intelligence/forensic-scientists-have-a-new-fingerprint-tool-in-their-arsenal-thanks-to-ai-but-its-sparked-a-controversy)

> Simon Cole ... is among those who say similarities in the fingerprints from different digits of the same person have long been known,
> even if forensic experts were unable to match them with the certainty required by courts.

> Furthermore, as law enforcement routinely takes prints from all ten digits, Cole said he can only see the technology having "rare and limited use" — linking
> separate prints from different crime scenes for suspects without all ten digits recorded.
